{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comic Crafter with LocalTunnel\n",
    "\n",
    "This notebook demonstrates how to run the Comic Crafter application in Google Colab using LocalTunnel for port exposure.\n",
    "Uses Open Router with free meta-llama/llama-3.3-70b-instruct for story generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Node.js and localtunnel\n",
    "!curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "!sudo apt-get install -y nodejs\n",
    "!npm install -g localtunnel\n",
    "\n",
    "# Install Python dependencies\n",
    "!pip install torch diffusers transformers accelerate Pillow Flask numpy requests openai\n",
    "!pip install safetensors # Required for loading LORA weights safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Open Router API key\n",
    "# Get your free API key from: https://openrouter.ai/keys\n",
    "import os\n",
    "os.environ['OPENROUTER_API_KEY'] = 'YOUR_OPENROUTER_API_KEY_HERE'  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and set up the model weights (if they don't exist already)\n",
    "import os\n",
    "os.makedirs('./models/weights', exist_ok=True)\n",
    "\n",
    "# Download sample LORA weights (or use default Stable Diffusion model)\n",
    "# This is a placeholder for actual model download - you can customize this\n",
    "print(\"Models directory prepared at ./models/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minimal server app in the current directory\n",
    "server_code = '''\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL_ID = os.getenv(\"SD_BASE_MODEL_ID\", \"runwayml/stable-diffusion-v1-5\")  # Base model\n",
    "LORA_MODEL_PATH = os.getenv(\"SD_LORA_PATH\", \"./models/weights\")  # Path to LORA weights\n",
    "LORA_WEIGHT_NAME = os.getenv(\"SD_LORA_WEIGHT\", \"pytorch_lora_weights.safetensors\")  # LORA weight filename\n",
    "CACHE_DIR = os.getenv(\"SD_CACHE_DIR\", \"./cache\")\n",
    "OUTPUT_DIR = os.getenv(\"SD_OUTPUT_DIR\", \"./outputs\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Global variables for model\n",
    "pipe = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the Stable Diffusion model with LORA weights\"\"\"\n",
    "    global pipe\n",
    "\n",
    "    logger.info(f\"Loading base model: {BASE_MODEL_ID}\")\n",
    "    try:\n",
    "        # Load the base model\n",
    "        pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            cache_dir=CACHE_DIR\n",
    "        )\n",
    "\n",
    "        # Load LORA weights if they exist\n",
    "        lora_full_path = os.path.join(LORA_MODEL_PATH, LORA_WEIGHT_NAME)\n",
    "        if os.path.exists(lora_full_path):\n",
    "            logger.info(f\"Loading LORA weights from {lora_full_path}\")\n",
    "            pipe.load_lora_weights(\n",
    "                LORA_MODEL_PATH,\n",
    "                weight_name=LORA_WEIGHT_NAME,\n",
    "                adapter_name=\"comic_style\"  # Named adapter for comic/manga style\n",
    "            )\n",
    "            logger.info(\"LORA weights loaded successfully\")\n",
    "        else:\n",
    "            logger.warning(f\"LORA weights not found at {lora_full_path}, loading base model only\")\n",
    "\n",
    "        # Move to GPU if available, otherwise CPU\n",
    "        if torch.cuda.is_available():\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            logger.info(\"Model loaded on GPU\")\n",
    "        else:\n",
    "            pipe = pipe.to(\"cpu\")\n",
    "            logger.info(\"Model loaded on CPU\")\n",
    "\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "@app.route(\"/sdapi/v1/txt2img\", methods=[\"POST\"])\n",
    "def txt2img():\n",
    "    \"\"\"Generate image from text prompt\"\"\"\n",
    "    global pipe\n",
    "\n",
    "    if pipe is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 500\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Extract parameters\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        negative_prompt = data.get(\"negative_prompt\", \"\")\n",
    "        steps = data.get(\"steps\", 20)\n",
    "        width = data.get(\"width\", 512)\n",
    "        height = data.get(\"height\", 512)\n",
    "        cfg_scale = data.get(\"cfg_scale\", 7.5)\n",
    "        seed = data.get(\"seed\", -1)\n",
    "        sampler_name = data.get(\"sampler_name\", \"Euler a\")\n",
    "\n",
    "        # Enhance the prompt with specific Japanese manga style instructions\n",
    "        enhanced_prompt = f\"Japanese manga style, {prompt}, highly detailed, black and white style, sharp lines, manga art style, professional quality, clean lines, detailed character design\"\n",
    "\n",
    "        # Enhanced negative prompt with manga-specific elements to avoid\n",
    "        enhanced_negative_format = f\"{negative_prompt}, color image, western cartoon style, low detail, blurry, deformed, ugly, anime screencap, digital art that looks like a screenshot\"\n",
    "\n",
    "        # Generate the image\n",
    "        generator = None\n",
    "        if seed != -1:\n",
    "            generator = torch.Generator(device=pipe.device).manual_seed(seed)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image = pipe(\n",
    "                prompt=enhanced_prompt,\n",
    "                negative_prompt=enhanced_negative_format,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=cfg_scale,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                generator=generator,\n",
    "                cross_attention_kwargs={\"scale\": 0.8}  # Apply LORA scaling if available\n",
    "            ).images[0]\n",
    "\n",
    "        # Save the image\n",
    "        output_filename = f\"{str(uuid.uuid4())}.png\"\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "        image.save(output_path)\n",
    "\n",
    "        # Convert to base64 for API response\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        return jsonify({\n",
    "            \"images\": [img_str],\n",
    "            \"parameters\": {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"steps\": steps,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"cfg_scale\": cfg_scale,\n",
    "                \"seed\": seed\n",
    "            },\n",
    "            \"info\": \"Image generated successfully with LORA weights\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in txt2img: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/sdapi/v1/img2img\", methods=[\"POST\"])\n",
    "def img2img():\n",
    "    \"\"\"Generate image from image and text prompt\"\"\"\n",
    "    global pipe\n",
    "\n",
    "    if pipe is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 500\n",
    "\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Extract parameters\n",
    "        init_images = data.get(\"init_images\", [])\n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        negative_prompt = data.get(\"negative_prompt\", \"\")\n",
    "        steps = data.get(\"steps\", 20)\n",
    "        denoising_strength = data.get(\"denoising_strength\", 0.75)\n",
    "        width = data.get(\"width\", 512)\n",
    "        height = data.get(\"height\", 512)\n",
    "        cfg_scale = data.get(\"cfg_scale\", 7.5)\n",
    "\n",
    "        if not init_images:\n",
    "            return jsonify({\"error\": \"No init_images provided\"}), 400\n",
    "\n",
    "        # Enhance the prompt with specific Japanese manga style instructions\n",
    "        enhanced_prompt = f\"Japanese manga style, {prompt}, highly detailed, black and white style, sharp lines, manga art style, professional quality, clean lines, detailed character design\"\n",
    "\n",
    "        # Enhanced negative prompt with manga-specific elements to avoid\n",
    "        enhanced_negative_format = f\"{negative_prompt}, color image, western cartoon style, low detail, blurry, deformed, ugly, anime screencap, digital art that looks like a screenshot\"\n",
    "\n",
    "        # Decode the first image\n",
    "        img_data = base64.b64decode(init_images[0])\n",
    "        init_image = Image.open(io.BytesIO(img_data)).convert(\"RGB\")\n",
    "\n",
    "        # Resize image to match dimensions\n",
    "        init_image = init_image.resize((width, height))\n",
    "\n",
    "        # Use img2img pipeline\n",
    "        with torch.no_grad():\n",
    "            image = pipe(\n",
    "                prompt=enhanced_prompt,\n",
    "                negative_prompt=enhanced_negative_format,\n",
    "                image=init_image,\n",
    "                num_inference_steps=int(steps / denoising_strength),  # Adjust steps based on denoising strength\n",
    "                guidance_scale=cfg_scale,\n",
    "                strength=denoising_strength,\n",
    "                generator=torch.Generator(device=pipe.device).manual_seed(42),  # Fixed seed for consistency\n",
    "                cross_attention_kwargs={\"scale\": 0.8}  # Apply LORA scaling if available\n",
    "            ).images[0]\n",
    "\n",
    "        # Convert to base64 for API response\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        return jsonify({\n",
    "            \"images\": [img_str],\n",
    "            \"parameters\": {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"steps\": steps,\n",
    "                \"denoising_strength\": denoising_strength,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"cfg_scale\": cfg_scale\n",
    "            },\n",
    "            \"info\": \"Image transformed successfully with LORA weights\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in img2img: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/sdapi/v1/options\", methods=[\"POST\"])\n",
    "def set_options():\n",
    "    \"\"\"Set Stable Diffusion options (stub implementation)\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        logger.info(f\"Setting options: {data}\")\n",
    "        return jsonify({\"status\": \"options set\"})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in set_options: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/sdapi/v1/sd-models\", methods=[\"GET\"])\n",
    "def get_models():\n",
    "    \"\"\"Get available models (stub implementation)\"\"\"\n",
    "    try:\n",
    "        return jsonify([{\n",
    "            \"title\": \"Comic Style Model (with LORA)\",\n",
    "            \"model_name\": \"comic-style-lora\",\n",
    "            \"hash\": \"lora-enhanced\"\n",
    "        }])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in get_models: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/story/generate\", methods=[\"POST\"])\n",
    "def generate_story():\n",
    "    \"\"\"Generate a story using Open Router with Llama 3.3 70B model\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
        \n",
    "        prompt = data.get(\"prompt\", \"\")\n",
    "        max_tokens = data.get(\"max_tokens\", 500)\n",
    "        temperature = data.get(\"temperature\", 0.7)\n",
    "        \n",
    "        # Get Open Router API key from environment variable\n",
    "        openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        if not openrouter_api_key:\n",
    "            return jsonify({\"error\": \"OPENROUTER_API_KEY environment variable not set\"}), 500\n",
    "        \n",
    "        # Prepare the request to Open Router\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {openrouter_api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": os.getenv(\"APP_URL\", \"http://localhost:5000\"),\n",
    "            \"X-Title\": \"Comic Crafter\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a creative storyteller that specializes in generating engaging, imaginative stories for comic books. Your stories should be vivid, detailed, and perfect for adaptation into visual comics.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Open Router API error: {response.text}\")\n",
    "            return jsonify({\"error\": f\"Story generation failed with status {response.status_code}\"}), 500\n",
    "        \n",
    "        result = response.json()\n",
    "        story_text = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        return jsonify({\n",
    "            \"story\": story_text,\n",
    "            \"model_used\": \"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "            \"tokens_used\": result.get('usage', {}).get('total_tokens', 0)\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_story: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    # Check if LORA weights are loaded by checking if the adapter is active\n",
    "    lora_loaded = False\n",
    "    if hasattr(pipe, 'lora_manager') and pipe.lora_manager:\n",
    "        lora_loaded = True\n",
    "    elif hasattr(pipe, 'loaded_lora'):\n",
    "        lora_loaded = pipe.loaded_lora is not None\n",
    "\n",
    "    return jsonify({\n",
    "        \"status\": \"ok\",\n",
    "        \"model_loaded\": pipe is not None,\n",
    "        \"lora_loaded\": lora_loaded,\n",
    "        \"story_generation_enabled\": bool(os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "    })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model when starting the service\n",
    "    load_model()\n",
    "\n",
    "    # Get port from environment variable or default to 5000\n",
    "    port = int(os.getenv(\"SD_API_PORT\", 5000))\n",
    "    host = os.getenv(\"SD_API_HOST\", \"0.0.0.0\")\n",
    "\n",
    "    logger.info(f\"Starting Stable Diffusion API with LORA on {host}:{port}\")\n",
    "    app.run(host=host, port=port, debug=False)\n",
    "'''\n",
    "\n",
    "# Write the server code to app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(server_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the backend server in background\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Start server in background\n",
    "server_process = subprocess.Popen([\"python\", \"app.py\"])\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(10)\n",
    "print(f\"Server started with PID: {server_process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Expose the server with localtunnel and display clickable link\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def wait_for_and_display_url(port, title=\"Comic Crafter Service\"):\n",
    "    \"\"\"Monitor localtunnel output and display the URL as a clickable link in Colab.\"\"\"\n",
    "    def monitor_lt():\n",
    "        # Start localtunnel and capture its output\n",
    "        cmd = ['lt', '--port', str(port)]\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        \n",
    "        # Wait for the URL to appear in the output\n",
    "        while True:\n",
    "            output = process.stderr.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                output = output.strip()\n",
    "                print(output)\n",
    "                \n",
    "                # Look for the localtunnel URL\n",
    "                if 'your url is:' in output.lower() or 'public url:' in output.lower():\n",
    "                    # Extract URL using regex\n",
    "                    url_match = re.search(r'(https?://[\\\\w.-]+)', output)\n",
    "                    if url_match:\n",
    "                        url = url_match.group(1)\n",
    "                        \n",
    "                        # Create clickable link in Colab\n",
    "                        try:\n",
    "                            from IPython.display import display, HTML\n",
    "                            import google.colab\n",
    "                            \n",
    "                            # Create a clickable link\n",
    "                            html_link = f'<p><a href=\"{url}\" target=\"_blank\">{title}: {url}</a></p>'\n",
    "                            display(HTML(html_link))\n",
    "                        except ImportError:\n",
    "                            # Not in Colab, just print the URL\n",
    "                            print(f\"{title}: {url}\")\n",
    "                        \n",
    "                        break\n",
    "    \n",
    "    # Start monitoring in a separate thread\n",
    "    monitor_thread = threading.Thread(target=monitor_lt)\n",
    "    monitor_thread.daemon = True\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    return monitor_thread\n",
    "\n",
    "# Display a clickable link for the service\n",
    "tunnel_thread = wait_for_and_display_url(5000)\n",
    "\n",
    "# Keep the cell running to maintain the tunnel\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Tunnel closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Make sure to copy the localtunnel URL that appears as a clickable link above\n",
    "2. The backend API URL will be used in the frontend configuration\n",
    "3. After kernel restart, you'll need to run these cells again\n",
    "4. LocalTunnel URLs are temporary and will change each time you restart the tunnel\n",
    "5. Remember to set your Open Router API key in the designated cell\n",
    "6. To use the story generation endpoint, call POST /story/generate with your prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}