{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comic Crafter with LocalTunnel\n",
    "\n",
    "This notebook demonstrates how to run the Comic Crafter application in Google Colab using LocalTunnel for port exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Git LFS if needed and clone the repository\n",
    "!git clone https://github.com/[your-username]/comic-crafter.git\n",
    "%cd comic-crafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Node.js and localtunnel\n",
    "!curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
    "!sudo apt-get install -y nodejs\n",
    "!npm install -g localtunnel\n",
    "\n",
    "# Install Python dependencies\n",
    "!pip install -r server/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the base model with LORA weights\n",
    "%cd utils\n",
    "!python setup_sd.py\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the backend server in background\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Start server in background\n",
    "server_process = subprocess.Popen([\"python\", \"server/app.py\"])\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(5)\n",
    "print(f\"Server started with PID: {server_process.pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expose the server with localtunnel\n",
    "# Note: This command will print the public URL to the console\n",
    "!lt --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Ollama for additional LLM capabilities\n",
    "# Install lshw first\n",
    "!apt-get install -y lshw\n",
    "\n",
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Start Ollama in background with nohup\n",
    "!nohup ollama serve > ollama.log 2>&1 &\n",
    "time.sleep(5)\n",
    "print(\"Ollama server started in background\")\n",
    "\n",
    "# Pull a model in background\n",
    "!nohup ollama pull llama3 > pull_llama3.log 2>&1 &\n",
    "print(\"Started pulling llama3 model in background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the frontend\n",
    "%cd client\n",
    "!npm install\n",
    "!npm run build\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally serve the frontend with localtunnel as well\n",
    "import http.server\n",
    "import socketserver\n",
    "from threading import Thread\n",
    "\n",
    "# Serve the built frontend\n",
    "PORT_FRONTEND = 3000\n",
    "DIRECTORY = \"client/dist\"\n",
    "\n",
    "class Handler(http.server.SimpleHTTPRequestHandler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, directory=DIRECTORY, **kwargs)\n",
    "\n",
    "def start_frontend_server():\n",
    "    with socketserver.TCPServer((\"\", PORT_FRONTEND), Handler) as httpd:\n",
    "        httpd.serve_forever()\n",
    "\n",
    "# Start frontend server in background\n",
    "frontend_thread = Thread(target=start_frontend_server, daemon=True)\n",
    "frontend_thread.start()\n",
    "print(f\"Frontend server started on port {PORT_FRONTEND}\")\n",
    "\n",
    "# Expose frontend with localtunnel\n",
    "print(\"Exposing frontend with localtunnel...\")\n",
    "!lt --port 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Make sure to copy the localtunnel URLs that appear in the output above\n",
    "2. The backend API URL will be used in the frontend configuration\n",
    "3. After kernel restart, you'll need to run these cells again\n",
    "4. LocalTunnel URLs are temporary and will change each time you restart the tunnel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}